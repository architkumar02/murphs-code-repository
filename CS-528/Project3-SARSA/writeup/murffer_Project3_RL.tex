\documentclass[conference]{IEEEtran}

% *** MISC UTILITY PACKAGES ***
%


% *** CITATION PACKAGES ***
%
\usepackage{cite}

% *** GRAPHICS RELATED PACKAGES ***
%
\usepackage[pdftex]{graphicx}
% declare the path(s) where your graphic files are
\graphicspath{{./images/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

% *** MATH PACKAGES ***
%
\usepackage[cmex10]{amsmath}

% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
\usepackage{algpseudocode}
% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}


% *** SUBFIGURE PACKAGES ***
\usepackage[cpation=false,font=footnotesize]{subfig}

% *** FLOAT PACKAGES ***
%

% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}


% correct bad hyphenation here
\hyphenation{}


\begin{document}
%
% paper title
\title{Wall following and obstacle avoidance with SARSA $\lambda$ Reinforcement Learning}


% author names and affiliations
\author{\IEEEauthorblockN{Matthew J. Urffer}
\IEEEauthorblockA{Department of Nuclear Engineering \\
University of Tennessee \\
Knoxville, Tennessee, 37916 \\
Email: matthew.urffer@gmail.com
}}



% make the title area
\maketitle


\begin{abstract}
\boldmath
Reinforcement learning is a learning process in which an agent is reward for making the correct actions at a particular state. 
This learning strategy is extremely useful in complex environments that are difficult to modeled, or ones in which the easement of an action is delayed.
This work attempts to implement a reinforcement learning strategy for a wall-following and obstacle avoidance robot in the player stage environment.
Attempts were made to implement reinforcement learning using the SARSA $\lambda$ algorithm with an $\epsilon$ greedy policy and eligibility traces, but ultimately these attempts did not result in a convergent implementation.
In this implementation a discredited state space was utilized as well as a discrete action space.
\end{abstract}
\IEEEpeerreviewmaketitle



\section{Introduction}

Reinforcement learning is a popular learning strategy for which direct supervision of the agent is not possible\cite{poliscuk_adaptive_2002,szepesvari_algorithms_2010}. 
Reinforcement learning is often formulated as a problem of trying to find the action that an agent can take in some environment in order to maximize a reward.
Rather than learning a particular solution to the problem, Reinforcement learning can bet thought as attempting to find a policy for solving the problem.  

\subsection{Reinforcement Learning}
A basic reinforcement learning problem is defined by\cite{poliscuk_adaptive_2002}:
\begin{itemize}
    \item a set of states $S$,
    \item a set of actions $A$,
    \item transitions between states,
    \item a reward function,
    \item and some terminal states.
\end{itemize}
Reinforcement learning algorithms then define an agent which goes out in the environment and is trained to complete the learning. 
The \emph{agent} in reinforcement must complete the following\cite{poliscuk_adaptive_2002}:
\begin{itemize}
	\item find out the \emph{state} of the environment,
	\item take \emph{actions} to modify the environment,
	\item determine if the \emph{goal} has been achieved.
\end{itemize}
The agent then interacts within the environment in order to achieve a goal.  In reinforcement learning this is broken into four sections:
\begin{itemize}
	\item a \emph{policy} which maps between a state and and action that the agent can preform,
	\item a \emph{reward function} which defines the goal of the agent,
	\item a \emph{value function} which determines how profitable it is to be in a given state (in relationship to achieving the award)
	\item and a \emph{environment} which determines the states and actions that can be completed.
\end{itemize}

\begin{figure}
\centering
\includegraphics[width=2.5in]{RLDiagram}
\caption{Reinforcement Learning Process}
\label{RLDiagram}
\end{figure}

\section{Methods}
\subsection{State and Action Formulation}
The problem was formulated as a wall following and obstacle avoidance problem in which it was desired to have the robot follow at least $d_{wall} = 0.75 \text{m}$ from a wall, and for the robot to come no closer than $d_{obstacle} = 0.25 \text{m}$ from an obstacle.
The player-stage simulation environment\cite{gerkey_player_2012} (a well known robotics simulation package) was chosen to simulate the actions and environments.  The states were discretized into a minimum left distance, a minimum right distance, and the distance to the closest object.
The actions that the robot could preform was also discretized.  
The robot could either move forward a set velocity (0.1 m/s), or rotate through 360 degrees in 30 degree incements.
The robot could not pick up or move objects.
\subsection{Reward Function}
The reward fucntion is necessary to provide feedback to the robot for being in a certian state.
As the objective is to follow a wall while avoiding obstacles the reward fucntion was designed to penalize the robot (based on a linear scale) on it's distance from the wall or obstalce.   
For example, if the robot was right next the wall (an maximum range of 0.0 m) the reward for that state was -100.  
However, if it was 0.50 m (when the wall following distance was 0.75 m) the reward for that state was only -33.
A similar scheme was employed for the obstacle distances.
It should be noted that terminal states are greater thatn 1 m from a wall, or with a maximum range of of 0.0 m.
Postive awards were rewarded for each centimeter the robot traveled based on its odometer.
\subsection{SARSA $\lambda$}
The SARSA $\lambda$ algorthim \ref{SARSALambda} was implememented with tables. 
The control policy was initilized to be the previous best (or for the first one a random control policy) and then the robot was released into the world.
Using the $\epsilon$-greedy polciy an action was chosen.  The new state was observed, as well as the reward for that state. The control policy was updated to reflect this new change.  This was repeated until a terminal state was reached (i.e. the robot had a collision or strayed too far from the well).
\begin{figure}
\begin{algorithmic}[1]
\Procedure{SARSA $\lambda$}
\State Initilize $Q(s,a)$
\While{For each epsiode}
    \State Initialize $s$, $a$
    \While{each step of episode}
        \State Take action $a$
        \State Observe $r$,$s'$
        \State Choose $a'$ from $Q(s,a)$
        \Comment{Use $\epsion$ greedy}
        \State $\delta \gets r + \gamma Q(s',a') - Q(s,a)$
    \EndWhile
\EndWhile
\EndProcedure
\end{algorithmic}
\caption{SARSA $\lambda$ Algorithm}
\label{SARSALambda}
\end{figure}
\subsection{Exploration vs. Exploitation}
Reinforcement learning algorithms have to balance exploration (determining the rewards of new actions) vs exploitation (choosing the action that previously had the best reward). For this implementation an $\epsilon$-greedy policy was chosen; for probability $\epsilon$ the action was chosen to exploit the previous knowledge of the solution (i.e.e the previous best action), but for probability $1-\epsilon$ a random action was chosen.
\section{Future Work}
It is acknowledged that
\begin{itemize}
	\item What happens if we expand the discretization
	\item Can we make two policies, one for avoidance and one for wall following
\end{itemize}


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}



\section{Conclusion}
Reinforcement learning has been proven as a technique of learning a policy necessary to achieve a goal. 
Reinforcement learning (using the SARSA $\lambda$ algorithm) was attempted to be implemented in the player-stage environment in order to teach a robot the optimal policy to avoid walls and to avoid obstacles.
While the particular implementation was not successful at achieving this goal, the general 
The conclusion goes here.



% use section* for acknowledgement
\section*{Acknowledgment}
Once again I would like to thank Matthew Lish for his help.  His support and insight is gratefully acknowledgment.


% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Zotero}
\begin{thebibliography}{1}

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
  0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}


% that's all folks
\end{document}


